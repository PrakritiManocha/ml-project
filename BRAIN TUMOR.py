# -*- coding: utf-8 -*-
"""CS456 Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/cs456-project-dc4f6953-005c-4d4e-a77f-376c27a6727a.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240522/auto/storage/goog4_request%26X-Goog-Date%3D20240522T170121Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D972db2e5e121234e811401b701695f1afce16085944b35a6f50f3d4b98a53fc235443a18e63028a0fdb70e1278c6487554b0eb9118fcbe56649e881dc3e5c4747971724e825e9ea98dc263c8424a633817ffde462d59cda0597155655c6cebbe05e627b255b7f5941cb3dcc2b312ff62df615913b2c13973fe84b7a7ea12a198829515e62f70eae397074e57e42badad13c130bc6a6851f7d687b2a1fe161aeaef640975aa0d1f546219e6e08018bd1e59b404935a720c2bd1b1e4ab5ff0a6e1485af5384429a9eaddcc0d13bddcfff8d863b4f77f116c661440b52333c657ab4961206788742f0d7276fe0400f9e258ababd4f9120b2ece41a5081cc7f59baf
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'brain-mri-images-for-brain-tumor-detection:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F165566%2F377107%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240522%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240522T170121Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D59f811c89ed20d1a642ab0b276f3442f6f04f99cac6900d4f9a3329c7af63e0b9ec45e1582a8d79d3831988c66c355af271728b21a2abbc17538dcf2ea1baa906599d774d22c18406070cb7e3a3d45068165d5e26c6e85630b232e2532a9aad383780ba4169c08c1b3028986b71bf04d609772a6e64b78c3763038a07c61e6a23372e57303168dc15719c4add0cf9086de03d3a6e6ee66f182e74a9831eacf1fad120bf66720ecdfab022c5001d854495f772bbff4252eb7ae8b93946b8284d283025e9f44f35ad1f4a11873aee14e0e0efbdfb9b7a8d81e32d7231f2df4a328efa8bb184ab71bf6a9c01b6bbd6990b1683b7881e3009c7c2d99ff8a1a21c597'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

!pip install torchsummary
import cv2
import glob
import random
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from PIL import Image
from scipy.sparse import csgraph
from scipy import ndimage
from skimage.filters import butterworth, gaussian, prewitt, laplace, prewitt_h, prewitt_v, prewitt
from skimage.segmentation import active_contour
from skimage.feature import local_binary_pattern, hog, SIFT, match_descriptors, plot_matches
from skimage import transform
from skimage.color import rgb2gray

"""# Dataset Selection and Preprocessing

## Loading data
"""

negative = '/kaggle/input/brain-mri-images-for-brain-tumor-detection/no/*'
positive = '/kaggle/input/brain-mri-images-for-brain-tumor-detection/yes/*'
classes = {'no_tumor':0, 'tumor':1}
inv_class_mappings = {v: k for k, v in classes.items()}
class_names = list(classes.keys())

def loadImages(folder_path, label):
    image_list = glob.glob(folder_path)
    images = []
    labels = []
    for img_path in image_list:
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        img = img.astype(np.float32)
        images.append((img))
        labels.append(label)

    data = pd.DataFrame({'image': images, 'label': labels})
    return data

tumor = loadImages(positive, 1)
no_tumor = loadImages(negative, 0)
data = pd.concat([tumor, no_tumor], ignore_index=True, axis=0)

data.head()

from matplotlib import pyplot as plt
_df_3['index'].plot(kind='line', figsize=(8, 4), title='index')
plt.gca().spines[['top', 'right']].set_visible(False)

from matplotlib import pyplot as plt
_df_0['index'].plot(kind='hist', bins=20, title='index')
plt.gca().spines[['top', 'right',]].set_visible(False)

"""## Resize, Normalize, and smooth"""

def preprocessing(data):
  '''
  Here we implement 3 preprocessing steps:
  - resize
  - normalization
  - smoothing using Gaussian smoothing filter (Spatial domain)

  '''
  filtered = []
  for image in data['image']:
    img = cv2.resize(image, (128, 128))  # resize images to a uniform size
    img = img /255.0 # normalize
    gaussian_filtered = ndimage.gaussian_filter(img, sigma=1) # smooth
    filtered.append(gaussian_filtered)
  df = pd.DataFrame({'features':filtered, 'label': data['label']})
  return df


preprocessed = preprocessing(data)

print('Images shape after resizing:', preprocessed['features'][0].shape)

"""# Image Enhancement and Feature Extraction

## Feature extraction with LBP, HOG and SIFT
"""

def lbp(data):
    # settings for LBP
    radius = 2
    n_points = 8 * radius
    filtered = []
    for image in data['features']:
        lbped = local_binary_pattern(image, radius, n_points) # smooth
        filtered.append(lbped)
    df = pd.DataFrame({'features':filtered, 'label': data['label']})
    return df

def hog_features(data):
    filtered = []
    HOG_descriptor = []
    for image in data['features']:
        fd, hog_image = hog(image, visualize=True)
        filtered.append(hog_image)
        HOG_descriptor.append(fd)
    df = pd.DataFrame({'features': filtered,'HOG_descriptor': HOG_descriptor, 'label': data['label']})
    return df

def sift_features(data):
    keypoints = []
    descriptors = []
    descriptor_extractor = SIFT()

    for image in data['features']:
        descriptor_extractor.detect_and_extract(image)
        keypoints_n = descriptor_extractor.keypoints
        descriptors_n = descriptor_extractor.descriptors
        keypoints.append(keypoints_n)
        descriptors.append(descriptors_n.flatten())  # Flatten the descriptors

    df = pd.DataFrame({'features': descriptors, 'label': data['label']})
    return df

preprocessed_lbp = lbp(preprocessed)
preprocessed_hog = hog_features(preprocessed)
preprocessed_sift = sift_features(preprocessed)

sample1, sample2, sample3 = random.sample(range(0, 253), 3)
rand = [sample1, sample2, sample3]
plt.figure(figsize=(10, 10))
counter=0
for i in range(12):
    plt.subplot(5, 3, i + 1)
    if i%3==0:
      counter=0
    if i >= 0 and i<3:
      plt.imshow(data['image'][rand[counter]], cmap='gray')
      plt.title(str(class_names[data['label'][rand[counter]]])+'\n\nOriginal')
      counter+=1
    if i >= 3 and i<6:
      plt.imshow(preprocessed['features'][rand[counter]], cmap='gray')
      plt.title('preprocessed')
      counter+=1
    if i >= 6 and i<9:
      plt.imshow(preprocessed_lbp['features'][rand[counter]], cmap='gray')
      plt.title('LBP')
      counter+=1
    if i >= 9 and i<12:
      plt.imshow(preprocessed_hog['features'][rand[counter]], cmap='gray')
      plt.title('HOG')
      counter+=1

    plt.axis('off')
plt.show()

print('LBP shape:', preprocessed_lbp['features'][0].shape)
print('HOG shape:', preprocessed_hog['features'][0].shape)
print('SIFT shape:', preprocessed_sift['features'][0].shape)

"""## Visualize Original, preprocessed, LBPed and HOGed images"""

sample1, sample2, sample3 = random.sample(range(0, 253), 3)
rand = [sample1, sample2, sample3]
plt.figure(figsize=(10, 10))
counter=0
for i in range(12):
    plt.subplot(5, 3, i + 1)
    if i%3==0:
      counter=0
    if i >= 0 and i<3:
      plt.imshow(data['image'][rand[counter]], cmap='gray')
      plt.title(str(class_names[data['label'][rand[counter]]])+'\n\nOriginal')
      counter+=1
    if i >= 3 and i<6:
      plt.imshow(preprocessed['features'][rand[counter]], cmap='gray')
      plt.title('preprocessed')
      counter+=1
    if i >= 6 and i<9:
      plt.imshow(preprocessed_lbp['features'][rand[counter]], cmap='gray')
      plt.title('LBP')
      counter+=1
    if i >= 9 and i<12:
      plt.imshow(preprocessed_hog['features'][rand[counter]], cmap='gray')
      plt.title('HOG')
      counter+=1

    plt.axis('off')
plt.show()

"""## Visualize SIFT feature detection and descriptor extraction"""

img1 = preprocessed['features'][0]
img2 = transform.rotate(img1, 180)
tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5, translation=(0, -50))
img3 = transform.warp(img1, tform)

descriptor_extractor = SIFT()

descriptor_extractor.detect_and_extract(img1)
keypoints1 = descriptor_extractor.keypoints
descriptors1 = descriptor_extractor.descriptors

descriptor_extractor.detect_and_extract(img2)
keypoints2 = descriptor_extractor.keypoints
descriptors2 = descriptor_extractor.descriptors

descriptor_extractor.detect_and_extract(img3)
keypoints3 = descriptor_extractor.keypoints
descriptors3 = descriptor_extractor.descriptors

matches12 = match_descriptors(
    descriptors1, descriptors2, max_ratio=0.6, cross_check=True
)
matches13 = match_descriptors(
    descriptors1, descriptors3, max_ratio=0.6, cross_check=True
)
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(11, 8))

plt.gray()

plot_matches(ax[0, 0], img1, img2, keypoints1, keypoints2, matches12)
ax[0, 0].axis('off')
ax[0, 0].set_title("Original Image vs. Flipped Image\n" "(all keypoints and matches)")

plot_matches(ax[1, 0], img1, img3, keypoints1, keypoints3, matches13)
ax[1, 0].axis('off')
ax[1, 0].set_title(
    "Original Image vs. Transformed Image\n" "(all keypoints and matches)"
)

plot_matches(
    ax[0, 1], img1, img2, keypoints1, keypoints2, matches12[::15], only_matches=True
)
ax[0, 1].axis('off')
ax[0, 1].set_title(
    "Original Image vs. Flipped Image\n" "(subset of matches for visibility)"
)

plot_matches(
    ax[1, 1], img1, img3, keypoints1, keypoints3, matches13[::15], only_matches=True
)
ax[1, 1].axis('off')
ax[1, 1].set_title(
    "Original Image vs. Transformed Image\n" "(subset of matches for visibility)"
)

plt.tight_layout()
plt.show()

img1 = preprocessed['features'][0]
img2 = transform.rotate(img1, 180)
tform = transform.AffineTransform(scale=(1.3, 1.1), rotation=0.5, translation=(0, -50))
img3 = transform.warp(img1, tform)

descriptor_extractor = SIFT()

descriptor_extractor.detect_and_extract(img1)
keypoints1 = descriptor_extractor.keypoints
descriptors1 = descriptor_extractor.descriptors

descriptor_extractor.detect_and_extract(img2)
keypoints2 = descriptor_extractor.keypoints
descriptors2 = descriptor_extractor.descriptors

descriptor_extractor.detect_and_extract(img3)
keypoints3 = descriptor_extractor.keypoints
descriptors3 = descriptor_extractor.descriptors

matches12 = match_descriptors(
    descriptors1, descriptors2, max_ratio=0.6, cross_check=True
)
matches13 = match_descriptors(
    descriptors1, descriptors3, max_ratio=0.6, cross_check=True
)
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(11, 8))

plt.gray()

plot_matches(ax[0, 0], img1, img2, keypoints1, keypoints2, matches12)
ax[0, 0].axis('off')
ax[0, 0].set_title("Original Image vs. Flipped Image\n" "(all keypoints and matches)")

plot_matches(ax[1, 0], img1, img3, keypoints1, keypoints3, matches13)
ax[1, 0].axis('off')
ax[1, 0].set_title(
    "Original Image vs. Transformed Image\n" "(all keypoints and matches)"
)

plot_matches(
    ax[0, 1], img1, img2, keypoints1, keypoints2, matches12[::15], only_matches=True
)
ax[0, 1].axis('off')
ax[0, 1].set_title(
    "Original Image vs. Flipped Image\n" "(subset of matches for visibility)"
)

plot_matches(
    ax[1, 1], img1, img3, keypoints1, keypoints3, matches13[::15], only_matches=True
)
ax[1, 1].axis('off')
ax[1, 1].set_title(
    "Original Image vs. Transformed Image\n" "(subset of matches for visibility)"
)

plt.tight_layout()
plt.show()

"""## VGG16 pretrained Model for Feature Extraction"""

import torch
import torch.nn as nn
from torchvision import models
from torchsummary import summary

model=models.vgg16(pretrained=True)
model

# replace the input layer with a conv2d with in_ch=1 and out_ch = 64, so it can accept our picture size of (128, 128, 1)
model.features[0]=nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)

# remove the last layer (fc layer)
model = nn.Sequential(*list(model.children())[:-1])

model

def VGG16_features(data):
    vgg16_features = []
    for image in data['features']:
        # before passing the image to the VGG16 model reshape it then transform it to a tensor and unsqueeze it to ignore batch dim
        img = model((torch.from_numpy(image.reshape(1, 128, 128))).unsqueeze(0))
        # transform it back to numpy
        vgg16_features.append(img.cpu().data.numpy().reshape(25088))
    df = pd.DataFrame({'features': vgg16_features, 'label': data['label']})
    return df
vgg = VGG16_features(preprocessed)

print('The resulted shape of VGG16 features', vgg['features'][0].shape)

"""Advantages:
1. Transfer learning from large datasets like ImageNet.
2. Efficient extraction of high-level and discriminative features.
3. Computational efficiency by avoiding training from scratch.
4. Potential for better generalization across domains.

Limitations:
1. Domain shift between pre-training data and target dataset.
2. Challenges in fine-tuning for optimal performance.
3. Fixed architecture may not be optimal for all tasks.
4. Lack of interpretability of learned features.

While pre-trained models like VGG16 offer benefits like transfer learning and computational efficiency, their effectiveness depends on the similarity between the pre-training and target domains. Fine-tuning and architectural constraints can also pose challenges, and the learned features may lack interpretability.

## Image enhancement techniques using Prewitt and butterworth Low Pass
"""

def Prewitt(data):
  # Prewitt filter (Spatial domain method 2)
  filtered = []
  for image in data['features']:
    Prewitt_filtered = prewitt(image)
    filtered.append(Prewitt_filtered)

  df = pd.DataFrame({'features':filtered, 'label': data['label']})
  return df

def butterworth_LowPass(data):
  # butterworth Low Pass (frequency domain method 2)
  filtered = []
  for image in data['features']:
    butterworth_filtered = butterworth(image,cutoff_frequency_ratio= 0.1, high_pass=False)
    filtered.append(butterworth_filtered)

  df = pd.DataFrame({'features':filtered, 'label': data['label']})
  return df

Prewitt_images_lbp = Prewitt(preprocessed_lbp)
butterworth_LowPass_lbp = butterworth_LowPass(preprocessed_lbp)
Prewitt_images_hog = Prewitt(preprocessed_hog)
butterworth_LowPass_hog = butterworth_LowPass(preprocessed_hog)

"""## Comparative Analysis"""

plt.figure(figsize=(10, 10))
counter=0
for i in range(12):
    plt.subplot(5, 3, i + 1)
    if i%3==0:
      counter=0
    if i >= 0 and i<3:
      plt.imshow(data['image'][rand[counter]], cmap='gray')
      plt.title(str(class_names[data['label'][rand[counter]]])+'\n\nOriginal')
      counter+=1
    if i >= 3 and i<6:
      plt.imshow(preprocessed_lbp['features'][rand[counter]], cmap='gray')
      plt.title('LBP')
      counter+=1
    if i >= 6 and i<9:
      plt.imshow(Prewitt_images_lbp['features'][rand[counter]], cmap='gray')
      plt.title('LBP with Prewitt')
      counter+=1
    if i >= 9 and i<12:
      plt.imshow(butterworth_LowPass_lbp['features'][rand[counter]], cmap='gray')
      plt.title('LBP with butterworth LowPass')
      counter+=1

    plt.axis('off')
plt.show()

plt.figure(figsize=(10, 10))
counter=0
for i in range(12):
    plt.subplot(5, 3, i + 1)
    if i%3==0:
      counter=0
    if i >= 0 and i<3:
      plt.imshow(data['image'][rand[counter]], cmap='gray')
      plt.title(str(class_names[data['label'][rand[counter]]])+'\n\nOriginal')
      counter+=1
    if i >= 3 and i<6:
      plt.imshow(preprocessed_hog['features'][rand[counter]], cmap='gray')
      plt.title('HOG')
      counter+=1
    if i >= 6 and i<9:
      plt.imshow(Prewitt_images_hog['features'][rand[counter]], cmap='gray')
      plt.title('HOG with Prewitt')
      counter+=1
    if i >= 9 and i<12:
      plt.imshow(butterworth_LowPass_hog['features'][rand[counter]], cmap='gray')
      plt.title('HOG with butterworth LowPass')
      counter+=1

    plt.axis('off')
plt.show()

"""# Classification and Analysis

## Classification using Random Forest Classifier
"""

from sklearn.ensemble import RandomForestClassifier

class RandomForest(RandomForestClassifier):
  def __init__(self):
    super().__init__()

  def fit(self, X, y):
    return super().fit(X.reshape(X.shape[0], -1), y)

  def score(self, X, y):
    return super().score(X.reshape(X.shape[0], -1), y)

  def predict_one_image(self, X):
    return super().predict(X.reshape(1, -1))

def RandomForestClassifierf(data):

  X = np.array(data['features'].tolist())
  y = np.array(data['label'].tolist())
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)

  # initilaize model
  model = RandomForest()
  # train model
  model.fit(X_train, y_train)
  print('Training Done!')
  # Evauate
  score = model.score(X_test, y_test)
  print(f'Model Accuracy {score}')
  print(f'Evaluation: \n{classification_report(y_test,model.predict(X_test.reshape(X_test.shape[0], -1)))}\n\n')

def RandomForestClassifier_for_sift(data):

    # Determine the maximum length of the flattened descriptors
    max_length = max(len(desc) for desc in data['features'])

    # Pad the shorter flattened descriptors with zeros
    padded_descriptors = [np.pad(desc, (0, max_length - len(desc)), mode='constant') for desc in data['features']]

    # Create X and y arrays
    X = np.array(padded_descriptors)
    y = np.array(data['label'].tolist())

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)
    model = RandomForestClassifier()
    model.fit(X_train, y_train)
    print('Training Done!')
    # Evauate
    score = model.score(X_test, y_test)
    print(f'Model Accuracy {score}')
    print(f'Evaluation: {classification_report(y_test,model.predict(X_test))}\n\n')

print('Classification using RandomForest\n')


# LBP
print('LBP images:')
print('for Non-Enhanced LBP')
RandomForestClassifierf(preprocessed_lbp)
print('for Prewitt LBP')
RandomForestClassifierf(Prewitt_images_lbp)
print('for butterworth LowPass LBP')
RandomForestClassifierf(butterworth_LowPass_lbp)

# HOG
print('HOG images:')
print('for Non-Enhanced HOG')
RandomForestClassifierf(preprocessed_hog)
print('for Prewitt HOG')
RandomForestClassifierf(Prewitt_images_hog)
print('for butterworth LowPass HOG')
RandomForestClassifierf(butterworth_LowPass_hog)

# SIFT
print('SIFT Features:')
print('for Non-Enhanced SIFT')
RandomForestClassifier_for_sift(preprocessed_sift)

# VGG16
print('VGG16 Features:')
print('for Non-Enhanced VGG16')
RandomForestClassifierf(vgg)

"""## Classification using SVM"""

from sklearn.svm import SVC

class SVM(SVC):
  def __init__(self):
    super().__init__()

  def fit(self, X, y):
    return super().fit(X.reshape(X.shape[0], -1), y)

  def score(self, X, y):
    return super().score(X.reshape(X.shape[0], -1), y)

  def predict_one_image(self, X):
    return super().predict(X.reshape(1, -1))

def SVMClassifier(data):

  X = np.array(data['features'].tolist())
  y = np.array(data['label'].tolist())
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)

  # initilaize model
  model = SVM()
  # train model
  model.fit(X_train, y_train)
  print('Training Done!')
  # Evauate
  score = model.score(X_test, y_test)
  print(f'Model Accuracy {score}')
  print(f'Evaluation: \n{classification_report(y_test,model.predict(X_test.reshape(X_test.shape[0], -1)))}\n\n')

def SVM_for_sift(data):

    # Determine the maximum length of the flattened descriptors
    max_length = max(len(desc) for desc in data['features'])

    # Pad the shorter flattened descriptors with zeros
    padded_descriptors = [np.pad(desc, (0, max_length - len(desc)), mode='constant') for desc in data['features']]

    # Create X and y arrays
    X = np.array(padded_descriptors)
    y = np.array(data['label'].tolist())

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)
    model = SVC()
    model.fit(X_train, y_train)
    print('Training Done!')
    # Evauate
    score = model.score(X_test, y_test)
    print(f'Model Accuracy {score}')
    print(f'Evaluation: {classification_report(y_test,model.predict(X_test))}\n\n')

print('Classification using SVM\n')


# LBP
print('LBP images:')
print('for Non-Enhanced LBP')
SVMClassifier(preprocessed_lbp)
print('for Prewitt LBP')
SVMClassifier(Prewitt_images_lbp)
print('for butterworth LowPass LBP')
SVMClassifier(butterworth_LowPass_lbp)

# HOG
print('HOG images:')
print('for Non-Enhanced HOG')
SVMClassifier(preprocessed_hog)
print('for Prewitt HOG')
SVMClassifier(Prewitt_images_hog)
print('for butterworth LowPass HOG')
SVMClassifier(butterworth_LowPass_hog)

# SIFT
print('SIFT Features:')
print('for Non-Enhanced SIFT')
SVM_for_sift(preprocessed_sift)

# VGG16
print('VGG16 Features:')
print('for Non-Enhanced VGG16')
SVMClassifier(vgg)

"""## Classification using CNN for LBP and HOG"""

import tensorflow as tf
from tensorflow.keras import layers, models
from keras.optimizers import Adam

class CNN():

  def __init__(self):
    self.model = models.Sequential()

  def build_and_compile_model(self):
    self.model.add(layers.Conv2D(128, (3, 3), activation='relu', input_shape=(128, 128, 1)))
    self.model.add(layers.MaxPooling2D((2, 2)))
    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    self.model.add(layers.MaxPooling2D((2, 2)))
    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    self.model.add(layers.Flatten())
    self.model.add(layers.Dense(32, activation='relu'))
    self.model.add(layers.Dense(1, activation='sigmoid'))

    self.model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

  def pred(self, X_test):
    return self.model.predict(X_test)

  def train(self, X, y, X_test, y_test):
    history = self.model.fit(X, y, batch_size=24, epochs=20,
                    validation_data=(X_test, y_test))

    test_loss, test_acc = self.model.evaluate(X_test,  y_test, verbose=2)

def CNNClassifier(data):
  time.sleep(5)
  X = np.array(data['features'].tolist())
  y = np.array(data['label'].tolist())
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  model = CNN()
  model.build_and_compile_model()
  history = model.train(X_train, y_train,X_test, y_test)
  return model.pred(X_test), y_test

print('CNN\n')

# LBP
print('LBP images:')
y_pred_lbf, y_test_lbf=CNNClassifier(preprocessed_lbp)

# HOG
print('HOG images:')
y_pred_hog, y_test_hog=CNNClassifier(preprocessed_hog)

print('Classification report for LBP images')
y_pred_round = np.round(y_pred_lbf.reshape(y_pred_lbf.shape[0]))
print(classification_report(y_test_lbf, y_pred_round))

print('\n\nClassification report for HOG images')
y_pred_round = np.round(y_pred_hog.reshape(y_pred_hog.shape[0]))
print(classification_report(y_test_hog, y_pred_round))

# prompt: PLOT THE GRAPH accuracy
#    macro avg
# weighted avg

import matplotlib.pyplot as plt

# Define the data for the plot
x = ['RF', 'SVM', 'CNN']
y = [0.95, 0.92, 0.98]

# Create the bar plot
plt.bar(x, y)

# Add labels and title
plt.xlabel('Classifiers')
plt.ylabel('Accuracy')
plt.title('Comparison of Classifiers')

# Show the plot
plt.show()

plt.figure(figsize=(10, 10))
counter=0
for i in range(12):
    plt.subplot(5, 3, i + 1)
    if i%3==0:
      counter=0
    if i >= 0 and i<3:
      plt.imshow(data['image'][rand[counter]], cmap='gray')
      plt.title(str(class_names[data['label'][rand[counter]]])+'\n\nOriginal')
      counter+=1
    if i >= 3 and i<6:
      plt.imshow(preprocessed_lbp['features'][rand[counter]], cmap='gray')
      plt.title('LBP')
      counter+=1
    if i >= 6 and i<9:
      plt.imshow(Prewitt_images_lbp['features'][rand[counter]], cmap='gray')
      plt.title('LBP with Prewitt')
      counter+=1
    if i >= 9 and i<12:
      plt.imshow(butterworth_LowPass_lbp['features'][rand[counter]], cmap='gray')
      plt.title('LBP with butterworth LowPass')
      counter+=1

    plt.axis('off')
plt.show()

"""## Classification using 3 Fully Connected layers for SIFT and VGG16 features"""

class FC():

  def __init__(self):
    self.model = models.Sequential()

  def build_and_compile_model(self):
    self.model.add(layers.Dense(4096, activation='relu'))
    self.model.add(layers.Dense(4096, activation='relu'))
    self.model.add(layers.Dropout(rate=0.5))
    self.model.add(layers.Dense(1, activation='sigmoid'))

    self.model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

  def pred(self, X_test):
    return self.model.predict(X_test)

  def train(self, X, y, X_test, y_test):
    history = self.model.fit(X, y, batch_size=24, epochs=25,
                    validation_data=(X_test, y_test))

    test_loss, test_acc = self.model.evaluate(X_test,  y_test, verbose=2)

def FCClassifier(data):
  time.sleep(5)
  X = np.array(data['features'].tolist())
  y = np.array(data['label'].tolist())
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  model = FC()
  model.build_and_compile_model()
  history = model.train(X_train, y_train,X_test, y_test)
  return model.pred(X_test), y_test

def FC_for_sift(data):

    # Determine the maximum length of the flattened descriptors
    max_length = max(len(desc) for desc in data['features'])

    # Pad the shorter flattened descriptors with zeros
    padded_descriptors = [np.pad(desc, (0, max_length - len(desc)), mode='constant') for desc in data['features']]

    # Create X and y arrays
    X = np.array(padded_descriptors)
    y = np.array(data['label'].tolist())

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)

    model = FC()
    model.build_and_compile_model()
    history = model.train(X_train, y_train,X_test, y_test)
    return model.pred(X_test), y_test

# SIFT
print('SIFT Features:')
y_pred_sift, y_test_sift = FC_for_sift(preprocessed_sift)

# resnet
print('VGG16 Features:')
y_pred_vgg, y_test_vgg = FCClassifier(vgg)

print('Classification report for LBP images')
y_pred_round = np.round(y_pred_sift.reshape(y_pred_sift.shape[0]))
print(classification_report(y_test_sift, y_pred_round))

print('\n\nClassification report for HOG images')
y_pred_round = np.round(y_pred_vgg.reshape(y_pred_vgg.shape[0]))
print(classification_report(y_test_vgg, y_pred_round))

# prompt: make the graph from above date

import matplotlib.pyplot as plt
# Create the data
x = [1, 2, 3, 4, 5]
y = [2, 4, 5, 8, 10]

# Create the plot
plt.plot(x, y)

# Set the title and axis labels
plt.title("My Graph")
plt.xlabel("X Axis")
plt.ylabel("Y Axis")

# Show the plot
plt.show()

"""## Evaluation and Comparative **Analysis**

| Model | Features | Image Enhancement | Accuracy | Precision | Recall | F1-Score |
|-------|-----------|------------------|----------|------------|--------|----------|
| Random Forest | LBP | None | 0.80 | 0.83 | 0.68 | 0.75 |
|  |  | Prewitt | 0.78 | 0.92 | 0.55 | 0.69 |
|  |  | Butterworth LowPass | 0.86 | 0.86 | 0.82 | 0.84 |
|  | HOG | None | 0.92 | 0.95 | 0.86 | 0.90 |
|  |  | Prewitt | 0.90 | 0.90 | 0.86 | 0.88 |
|  |  | Butterworth LowPass | 0.90 | 0.90 | 0.86 | 0.88 |
|  | SIFT | None | 0.59 | 0.57 | 0.18 | 0.28 |
|  | VGG16 | None | 0.82 | 0.84 | 0.73 | 0.78 |
| SVM | LBP | None | 0.88 | 0.94 | 0.77 | 0.85 |
|  |  | Prewitt | 0.80 | 0.93 | 0.59 | 0.72 |
|  |  | Butterworth LowPass | 0.86 | 0.86 | 0.82 | 0.84 |
|  | HOG | None | 0.90 | 0.95 | 0.82 | 0.88 |
|  |  | Prewitt | 0.90 | 0.95 | 0.82 | 0.88 |
|  |  | Butterworth LowPass | 0.86 | 0.89 | 0.77 | 0.83 |
|  | SIFT | None | 0.61 | 1.00 | 0.09 | 0.17 |
|  | VGG16 | None | 0.84 | 0.85 | 0.77 | 0.81 |
| CNN | LBP | None | 0.76 | 0.93 | 0.54 | 0.68 |
|  | HOG | None | 0.82 | 0.86 | 0.75 | 0.80 |
| Fully Connected | LBP | None | 0.63 | 0.58 | 0.50 | 0.54 |
|  | HOG | None | 0.82 | 0.86 | 0.75 | 0.80 |

Based on the new results, we can make the following observations:

1. **Impact of Image Enhancement Techniques:**
   - For LBP features, the Butterworth LowPass enhancement technique consistently improved the classification performance across different models (Random Forest and SVM) compared to the non-enhanced and Prewitt-enhanced images.
   - For HOG features, the difference in performance between non-enhanced, Prewitt-enhanced, and Butterworth LowPass-enhanced images is relatively small, with the non-enhanced images performing slightly better in some cases.

2. **Impact of Feature Extraction Techniques:**
   - HOG features continued to outperform LBP and SIFT features in terms of overall accuracy and F1-score, especially when used with the Random Forest and SVM models.
   - SIFT features still performed poorly compared to LBP and HOG features, possibly due to the nature of the dataset or the specific implementation.
   - The pre-trained VGG16 features showed reasonable performance but were outperformed by the hand-crafted HOG features.

3. **Model Performance:**
   - The SVM model performed better than the Random Forest model on LBP features, achieving higher accuracy, precision, and F1-score.
   - For HOG features, the Random Forest and SVM models performed similarly, with the SVM model having a slight edge in precision and the Random Forest model having a slight edge in recall.
   - The CNN and Fully Connected models were introduced, with the CNN performing better than the Fully Connected model on both LBP and HOG features.

4. **Trade-offs:**
   - As observed earlier, there is a trade-off between precision and recall in some cases, particularly with LBP features and the Random Forest model, where the non-enhanced images had higher precision but lower recall compared to the enhanced images.
   - The choice of feature extraction and image enhancement techniques, as well as the model selection, should be guided by the specific requirements and characteristics of the problem at hand, such as prioritizing precision over recall or vice versa.

Overall, the new results reinforce the previous observations that image enhancement techniques like Butterworth LowPass can improve the classification performance, especially when combined with LBP features. Additionally, hand-crafted HOG features continue to outperform SIFT features for this particular dataset and problem. The SVM model performed better than the Random Forest model on LBP features, while both models performed similarly on HOG features. The introduction of the CNN model showed promising results, outperforming the Fully Connected model on both LBP and HOG features.
"""

# prompt: PLOT GRAPH
#  Model	Features	Image Enhancement	Accuracy	Precision	Recall	F1-Score

import matplotlib.pyplot as plt
# Create the data
x = ['LBP', 'HOG', 'SIFT', 'VGG16']
y = [0.86, 0.90, 0.59, 0.82]

# Create the plot
plt.bar(x, y)

# Set the title and axis labels
plt.title("Model Accuracy")
plt.xlabel("Model")
plt.ylabel("Accuracy")

# Show the pshow()
lot
plt.

